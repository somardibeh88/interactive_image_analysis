{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import h5py\n",
    "from data_loader import ImageSequence\n",
    "\n",
    "def dict_to_json_serializable(d):\n",
    "    \"\"\"Recursively convert non-serializable values to strings\"\"\"\n",
    "    def _convert(value):\n",
    "        if isinstance(value, (str, int, float, bool, type(None))):\n",
    "            return value\n",
    "        elif isinstance(value, dict):\n",
    "            return {k: _convert(v) for k, v in value.items()}\n",
    "        elif isinstance(value, (list, tuple)):\n",
    "            return [_convert(v) for v in value]\n",
    "        else:\n",
    "            return str(value)  # Convert non-serializable objects to strings\n",
    "    return _convert(d)\n",
    "\n",
    "def save_stack_hdf5(directory, output_file=\"stack_testing.h5\"):\n",
    "    with h5py.File(os.path.join(directory, output_file), \"w\") as hf:\n",
    "        img_group = hf.create_group(\"images\")\n",
    "        meta_group = hf.create_group(\"metadata\")\n",
    "\n",
    "        valid_files = sorted([\n",
    "            f for f in os.listdir(directory) \n",
    "            if f.endswith('.ndata1') or f.endswith('tif') and 'SuperScan (HAADF) (Gaussian Blur)' not in f\n",
    "        and 'EELS' not in f])\n",
    "\n",
    "        for idx, filename in enumerate(valid_files):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            \n",
    "            # Load image data\n",
    "            imageseq = ImageSequence(file_path)\n",
    "            img = imageseq.raw_data\n",
    "            \n",
    "            # Store image with compression\n",
    "            img_ds = img_group.create_dataset(\n",
    "                name=f\"image_{idx:04d}\",\n",
    "                data=img,\n",
    "                compression=\"gzip\"\n",
    "            )\n",
    "            \n",
    "            # Handle metadata with fallback\n",
    "            try:\n",
    "                raw_meta = imageseq.raw_metadata or {}\n",
    "            except AttributeError:\n",
    "                raw_meta = {}\n",
    "                \n",
    "            # Convert to JSON-serializable format\n",
    "            processed_meta = dict_to_json_serializable(raw_meta)\n",
    "            \n",
    "            # Add fallback if empty\n",
    "            if not processed_meta:\n",
    "                processed_meta = {\"metadata_status\": \"no_metadata_found\"}\n",
    "                \n",
    "            # Store metadata as JSON string\n",
    "            meta_ds = meta_group.create_dataset(\n",
    "                name=f\"metadata_{idx:04d}\",\n",
    "                data=json.dumps(processed_meta),\n",
    "                dtype=h5py.string_dtype()\n",
    "            )\n",
    "            \n",
    "            # Create cross-reference\n",
    "            img_ds.attrs[\"metadata_ref\"] = f\"metadata_{idx:04d}\"\n",
    "            meta_ds.attrs[\"image_ref\"] = f\"image_{idx:04d}\"\n",
    "\n",
    "def process_directory(directory):\n",
    "    if any(f.endswith('.ndata1') or f.endswith('.tif') for f in os.listdir(directory)):\n",
    "        save_stack_hdf5(directory)\n",
    "    else:\n",
    "        for subdir in os.listdir(directory):\n",
    "            subdir_path = os.path.join(directory, subdir)\n",
    "            if os.path.isdir(subdir_path):\n",
    "                process_directory(subdir_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_directory('/home/somar/Desktop/2025/Data for publication/Sample 2344/ADF images/')\n",
    "    # process_directory('/home/somar/Desktop/2025/Data for publication/Multilayer graphene/')\n",
    "    process_directory('/home/somar/Downloads/Schrirang/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import h5py\n",
    "from data_loader import ImageSequence\n",
    "\n",
    "def dict_to_json_serializable(d):\n",
    "    def _convert(value):\n",
    "        if isinstance(value, (str, int, float, bool, type(None))):\n",
    "            return value\n",
    "        elif isinstance(value, dict):\n",
    "            return {k: _convert(v) for k, v in value.items()}\n",
    "        elif isinstance(value, (list, tuple)):\n",
    "            return [_convert(v) for v in value]\n",
    "        else:\n",
    "            return str(value)\n",
    "    return _convert(d)\n",
    "\n",
    "def save_stack_hdf5(directory, output_file=\"stack.h5\"):\n",
    "    valid_files = sorted([\n",
    "        f for f in os.listdir(directory)\n",
    "        if f.endswith('.ndata1') and 'SuperScan (HAADF) (Gaussian Blur)' not in f\n",
    "    ])\n",
    "\n",
    "    vcr_files = []\n",
    "    non_vcr_files = []\n",
    "\n",
    "    for filename in valid_files:\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        imageseq = ImageSequence(file_path)\n",
    "        if imageseq.raw_data.ndim == 3:\n",
    "            vcr_files.append((filename, imageseq))\n",
    "        else:\n",
    "            non_vcr_files.append((filename, imageseq))\n",
    "\n",
    "    if non_vcr_files:\n",
    "        with h5py.File(os.path.join(directory, output_file), \"w\") as hf:\n",
    "            img_group = hf.create_group(\"images\")\n",
    "            meta_group = hf.create_group(\"metadata\")\n",
    "\n",
    "            fov_data = []\n",
    "            for idx, (filename, imageseq) in enumerate(non_vcr_files):\n",
    "                try:\n",
    "                    raw_meta = imageseq.raw_metadata or {}\n",
    "                except AttributeError:\n",
    "                    raw_meta = {}\n",
    "                processed_meta = dict_to_json_serializable(raw_meta)\n",
    "                metadata = processed_meta.get('metadata', {})\n",
    "\n",
    "                fov = float('inf')\n",
    "                try:\n",
    "                    fov = float(metadata.get('scan', {}).get('scan_device_properties', {}).get('fov_nm', float('inf')))\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        fov = float(processed_meta.get('instrument', {}).get('ImageScanned', {}).get('fov_nm', float('inf')))\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "\n",
    "                fov_data.append((idx, fov, imageseq.raw_data, processed_meta))\n",
    "\n",
    "            fov_data.sort(key=lambda x: x[1])\n",
    "\n",
    "            for new_idx, (orig_idx, fov, img, meta) in enumerate(fov_data):\n",
    "                img_ds = img_group.create_dataset(\n",
    "                    f\"image_{new_idx:04d}\",\n",
    "                    data=img,\n",
    "                    compression=\"gzip\"\n",
    "                )\n",
    "                img_ds.attrs[\"fov_nm\"] = fov if fov != float('inf') else \"unknown\"\n",
    "\n",
    "                meta_ds = meta_group.create_dataset(\n",
    "                    f\"metadata_{new_idx:04d}\",\n",
    "                    data=json.dumps(meta),\n",
    "                    dtype=h5py.string_dtype()\n",
    "                )\n",
    "                img_ds.attrs[\"metadata_ref\"] = f\"metadata_{new_idx:04d}\"\n",
    "                meta_ds.attrs[\"image_ref\"] = f\"image_{new_idx:04d}\"\n",
    "                meta_ds.attrs[\"original_index\"] = orig_idx\n",
    "\n",
    "            hf.attrs[\"sorting\"] = \"fov_nm (unknown FOV at end)\"\n",
    "            hf.attrs[\"sorting_version\"] = \"1.0\"\n",
    "\n",
    "    for vcr_idx, (filename, imageseq) in enumerate(vcr_files, start=1):\n",
    "            vcr_output = os.path.join(directory, f\"VCR{vcr_idx}.h5\")\n",
    "            with h5py.File(vcr_output, \"w\") as vcr_hf:\n",
    "                img_group = vcr_hf.create_group(\"images\")\n",
    "                meta_group = vcr_hf.create_group(\"metadata\")\n",
    "\n",
    "                # Get raw metadata\n",
    "                try:\n",
    "                    raw_meta = imageseq.raw_metadata or {}\n",
    "                except AttributeError:\n",
    "                    raw_meta = {}\n",
    "                processed_meta = dict_to_json_serializable(raw_meta)\n",
    "                metadata = processed_meta.get('metadata', {})\n",
    "                # Extract timeseries data\n",
    "                timeseries = metadata.get('hardware_source', {}).get('timeseries', [])\n",
    "                img_data = imageseq.raw_data\n",
    "                num_images = img_data.shape[0]\n",
    "\n",
    "                # Create timestamp-FOV mapping\n",
    "                fov_map = []\n",
    "                for ts_entry in timeseries:\n",
    "                    fov_map.append((\n",
    "                        ts_entry.get('timestamp', float('inf')),\n",
    "                        ts_entry.get('FOV', float('inf'))\n",
    "                    ))\n",
    "\n",
    "                for i in range(num_images):\n",
    "                    # Clone base metadata\n",
    "                    image_meta = metadata.copy()\n",
    "                    \n",
    "                    # Find closest timestamp match\n",
    "                    if i < len(fov_map):\n",
    "                        # Direct index match if available\n",
    "                        ts, fov = fov_map[i]\n",
    "                        print(f\"Using field of view {fov} for image {i}\")\n",
    "                        metadata['scan']['scan_device_properties']['fov_nm'] = fov\n",
    "\n",
    "                    else:\n",
    "                        # Fallback: find nearest timestamp\n",
    "                        ts, fov = min(fov_map, key=lambda x: abs(x[0] - (fov_map[-1][0] if fov_map else 0)))\n",
    "                        \n",
    "                    # Update metadata with time-specific FOV\n",
    "                    image_meta['scan'] = image_meta.get('scan', {})\n",
    "                    image_meta['scan']['scan_device_properties'] = image_meta['scan'].get('scan_device_properties', {})\n",
    "                    image_meta['scan']['scan_device_properties']['fov_nm'] = fov\n",
    "\n",
    "                    # Store metadata\n",
    "                    meta_ds = meta_group.create_dataset(\n",
    "                        name=f\"metadata_{i:04d}\",\n",
    "                        data=json.dumps(image_meta),\n",
    "                        dtype=h5py.string_dtype()\n",
    "                    )\n",
    "\n",
    "                    # Store image\n",
    "                    img_slice = img_data[i]\n",
    "                    img_ds = img_group.create_dataset(\n",
    "                        f\"image_{i:04d}\",\n",
    "                        data=img_slice,\n",
    "                        compression=\"gzip\"\n",
    "                    )\n",
    "\n",
    "                    # Link metadata\n",
    "                    img_ds.attrs[\"metadata_ref\"] = f\"metadata_{i:04d}\"\n",
    "                    meta_ds.attrs[\"image_ref\"] = f\"image_{i:04d}\"\n",
    "                    meta_ds.attrs[\"timestamp\"] = ts\n",
    "                    meta_ds.attrs[\"fov_nm\"] = fov\n",
    "\n",
    "                # Add validation attributes\n",
    "                vcr_hf.attrs.update({\n",
    "                    \"is_vcr_stack\": True,\n",
    "                    \"num_images\": num_images,\n",
    "                    \"timeseries_entries\": len(timeseries),\n",
    "                    \"fov_mapping\": \"timestamp-matched\" if len(timeseries) >= num_images else \"fallback\",\n",
    "                    \"original_filename\": filename\n",
    "                })\n",
    "\n",
    "\n",
    "\n",
    "def process_directory(directory):\n",
    "    if any(f.endswith('.ndata1') for f in os.listdir(directory)):\n",
    "        save_stack_hdf5(directory)\n",
    "    else:\n",
    "        for subdir in os.listdir(directory):\n",
    "            subdir_path = os.path.join(directory, subdir)\n",
    "            if os.path.isdir(subdir_path):\n",
    "                process_directory(subdir_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_directory('/home/somar/lab course/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import h5py\n",
    "from data_loader import ImageSequence\n",
    "def dict_to_json_serializable(d):\n",
    "    \"\"\"Recursively convert non-serializable values to strings\"\"\"\n",
    "    def _convert(value):\n",
    "        if isinstance(value, (str, int, float, bool, type(None))):\n",
    "            return value\n",
    "        elif isinstance(value, dict):\n",
    "            return {k: _convert(v) for k, v in value.items()}\n",
    "        elif isinstance(value, (list, tuple)):\n",
    "            return [_convert(v) for v in value]\n",
    "        else:\n",
    "            return str(value)  # Convert non-serializable objects to strings\n",
    "    return _convert(d)\n",
    "\n",
    "def save_stack_hdf5(directory, output_file=\"stacktest.h5\"):\n",
    "    with h5py.File(os.path.join(directory, output_file), \"w\") as hf:\n",
    "        img_group = hf.create_group(\"images\")\n",
    "        meta_group = hf.create_group(\"metadata\")\n",
    "\n",
    "        valid_files = sorted([\n",
    "            f for f in os.listdir(directory) \n",
    "            if f.endswith('.ndata1') and 'SuperScan (HAADF) (Gaussian Blur)' not in f\n",
    "        ])\n",
    "\n",
    "        fov_data = []\n",
    "\n",
    "        for idx, filename in enumerate(valid_files):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            imageseq = ImageSequence(file_path)\n",
    "\n",
    "            try:\n",
    "                raw_meta = imageseq.raw_metadata or {}\n",
    "            except AttributeError:\n",
    "                raw_meta = {}\n",
    "\n",
    "            try:\n",
    "                metadata_dict = raw_meta[0] if hasattr(raw_meta, '__getitem__') else raw_meta\n",
    "            except Exception:\n",
    "                metadata_dict = {}\n",
    "\n",
    "            metadata = metadata_dict.get('metadata', {})\n",
    "\n",
    "            # Extract FOV with fallbacks\n",
    "            fov = float('inf')\n",
    "            try:\n",
    "                fov = float(metadata.get('scan', {}).get('scan_device_properties', {}).get('fov_nm', float('inf')))\n",
    "            except Exception:\n",
    "                try:\n",
    "                    fov = float(metadata_dict.get('instrument', {}).get('ImageScanned', {}).get('fov_nm', float('inf')))\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "\n",
    "            try:\n",
    "                img = imageseq.raw_data[0] \n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            fov_data.append((idx, fov, img, metadata_dict))\n",
    "\n",
    "        # Sort by FOV (lowest first; unknown at end)\n",
    "        fov_data.sort(key=lambda x: x[1])\n",
    "\n",
    "        for new_idx, (orig_idx, fov, img, meta) in enumerate(fov_data):\n",
    "            # Store image\n",
    "            img_ds = img_group.create_dataset(\n",
    "                name=f\"image_{new_idx:04d}\",\n",
    "                data=img,\n",
    "                compression=\"gzip\"\n",
    "            )\n",
    "            img_ds.attrs[\"fov_nm\"] = fov if fov != float('inf') else \"unknown\"\n",
    "\n",
    "            # Store metadata\n",
    "            meta_ds = meta_group.create_dataset(\n",
    "                name=f\"metadata_{new_idx:04d}\",\n",
    "                data=json.dumps(dict_to_json_serializable(meta)),\n",
    "                dtype=h5py.string_dtype()\n",
    "            )\n",
    "\n",
    "            # Cross-reference\n",
    "            img_ds.attrs[\"metadata_ref\"] = f\"metadata_{new_idx:04d}\"\n",
    "            meta_ds.attrs[\"image_ref\"] = f\"image_{new_idx:04d}\"\n",
    "            meta_ds.attrs[\"original_index\"] = orig_idx\n",
    "\n",
    "        hf.attrs[\"sorting\"] = \"fov_nm (unknown FOV at end)\"\n",
    "        hf.attrs[\"sorting_version\"] = \"1.0\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_directory(directory):\n",
    "    if any(f.endswith('.ndata1') for f in os.listdir(directory)):\n",
    "        save_stack_hdf5(directory)\n",
    "    else:\n",
    "        for subdir in os.listdir(directory):\n",
    "            subdir_path = os.path.join(directory, subdir)\n",
    "            if os.path.isdir(subdir_path):\n",
    "                process_directory(subdir_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # process_directory('/home/somar/Desktop/2025/Data for publication/')\n",
    "    process_directory('/home/somar/Desktop/2025/Data for publication/Multilayer graphene/')\n",
    "    # process_directory('/home/somar/Desktop/2025/Data for publication/Sample 2525/SSB reconstruction of 4d STEM data/')\n",
    "    # process_directory('/home/somar/Desktop/2025/Data for publication/Sample 2525/ADF images/')\n",
    "    # process_directory('/home/somar/test /')\n",
    "    process_directory('/home/somar/lab course/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import json\n",
    "from imagesequence import ImageSequence\n",
    "\n",
    "def serialize_metadata(meta):\n",
    "    \"\"\"Recursively converts metadata to JSON-serializable format\"\"\"\n",
    "    def _serialize(obj):\n",
    "        # Handles basic types\n",
    "        if isinstance(obj, (str, int, float, bool, type(None))):\n",
    "            return obj\n",
    "        # Recursively process dictionaries\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: _serialize(v) for k, v in obj.items()}\n",
    "        # Process lists/tuples\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            return [_serialize(v) for v in obj]\n",
    "        # Convert other types to strings\n",
    "        else:\n",
    "            return str(obj)\n",
    "    return _serialize(meta)\n",
    "\n",
    "def save_stack_hdf5(directory, output_file=\"stack.h5\"):\n",
    "    \"\"\"Main function to create HDF5 stack\"\"\"\n",
    "    with h5py.File(os.path.join(directory, output_file), \"w\") as hf:\n",
    "        # Get sorted list of valid files\n",
    "        valid_files = sorted([\n",
    "            f for f in os.listdir(directory)\n",
    "            if f.endswith('.ndata1') and 'SuperScan (HAADF) (Gaussian Blur)' not in f\n",
    "        ])\n",
    "\n",
    "        for idx, filename in enumerate(valid_files):\n",
    "            # Create unique group for each image\n",
    "            img_group = hf.create_group(f\"image_{idx:04d}\")\n",
    "            \n",
    "            # Load and store image data\n",
    "            imageseq = ImageSequence(os.path.join(directory, filename))\n",
    "            img_group.create_dataset(\"data\", \n",
    "                                   data=imageseq.raw_data, \n",
    "                                   compression=\"gzip\")\n",
    "            \n",
    "            # Handle metadata\n",
    "            try:\n",
    "                raw_meta = imageseq.raw_metadata or {}\n",
    "            except AttributeError:\n",
    "                raw_meta = {\"metadata_status\": \"no_metadata_found\"}\n",
    "            \n",
    "            # Store processed metadata as JSON string\n",
    "            processed_meta = serialize_metadata(raw_meta)\n",
    "            img_group.attrs.update({\n",
    "                \"metadata\": json.dumps(processed_meta),\n",
    "                \"original_filename\": filename\n",
    "            })\n",
    "\n",
    "def process_directory(directory):\n",
    "    \"\"\"Recursive directory processor\"\"\"\n",
    "    if any(f.endswith('.ndata1') for f in os.listdir(directory)):\n",
    "        save_stack_hdf5(directory)\n",
    "    else:\n",
    "        for subdir in os.listdir(directory):\n",
    "            subdir_path = os.path.join(directory, subdir)\n",
    "            if os.path.isdir(subdir_path):\n",
    "                process_directory(subdir_path)\n",
    "\n",
    "\n",
    "\n",
    "def load_image_with_metadata(h5_file, index):\n",
    "    with h5py.File(h5_file, \"r\") as hf:\n",
    "        img = hf[f\"images/image_{index:04d}\"][:]\n",
    "        meta_str = hf[f\"metadata/metadata_{index:04d}\"][()]\n",
    "        metadata = json.loads(meta_str)\n",
    "        return img, metadata\n",
    "# Example usage:\n",
    "img, metadata = load_image_with_metadata(\"stack.h5\", 0)\n",
    "img, metadata\n",
    "\n",
    "\n",
    "def load_full_stack(h5_file):\n",
    "    images = []\n",
    "    metadatas = []\n",
    "    with h5py.File(h5_file, \"r\") as hf:\n",
    "        # Get sorted group names\n",
    "        data_group = hf[\"images\"]\n",
    "        groups = sorted([k for k in data_group if k.startswith(\"image_\")], \n",
    "                       key=lambda x: int(x.split(\"_\")[1]))\n",
    "        print(groups)\n",
    "        metadata_group= hf[f\"metadata\"]\n",
    "        metadata = sorted([k for k in metadata_group if k.startswith(\"metadata_\")], \n",
    "                       key=lambda x: int(x.split(\"_\")[1]))\n",
    "        \n",
    "        for group_name in groups:\n",
    "            # print(f\"images/{group_name}\")\n",
    "            group = hf[f\"images/{group_name}\"]\n",
    "            images.append(group[:])\n",
    "\n",
    "        for group_name in metadata:\n",
    "            # print(f\"metadata/{group_name}\")\n",
    "            group = hf[f\"metadata/{group_name}\"]\n",
    "            metadatas.append(json.loads(group[()]))\n",
    "    return images, metadatas\n",
    "# Example usage\n",
    "h5_file = \"stack.h5\"\n",
    "images , metadata= load_full_stack(h5_file)\n",
    "len(images), len(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "\n",
    "def load_image_with_metadata(h5_file, index):\n",
    "    with h5py.File(h5_file, \"r\") as hf:\n",
    "        img = hf[f\"images/image_{index:04d}\"][:]\n",
    "        meta_str = hf[f\"metadata/metadata_{index:04d}\"][()]\n",
    "        metadata = json.loads(meta_str)\n",
    "        return img, metadata\n",
    "# Example usage:\n",
    "img, metadata = load_image_with_metadata(\"stack.h5\", 0)\n",
    "img, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "def load_full_stack(h5_file):\n",
    "    images = []\n",
    "    metadatas = []\n",
    "    with h5py.File(h5_file, \"r\") as hf:\n",
    "        # Get sorted group names\n",
    "        data_group = hf[\"images\"]\n",
    "        groups = sorted([k for k in data_group if k.startswith(\"image_\")], \n",
    "                       key=lambda x: int(x.split(\"_\")[1]))\n",
    "        print(groups)\n",
    "        metadata_group= hf[f\"metadata\"]\n",
    "        metadata = sorted([k for k in metadata_group if k.startswith(\"metadata_\")], \n",
    "                       key=lambda x: int(x.split(\"_\")[1]))\n",
    "        \n",
    "        for group_name in groups:\n",
    "            # print(f\"images/{group_name}\")\n",
    "            group = hf[f\"images/{group_name}\"]\n",
    "            images.append(group[:])\n",
    "\n",
    "        for group_name in metadata:\n",
    "            # print(f\"metadata/{group_name}\")\n",
    "            group = hf[f\"metadata/{group_name}\"]\n",
    "            metadatas.append(json.loads(group[()]))\n",
    "    return images, metadatas\n",
    "# Example usage\n",
    "h5_file = \"stack.h5\"\n",
    "images , metadata= load_full_stack(h5_file)\n",
    "len(images), len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "from imagesequence import ImageSequence\n",
    "\n",
    "def serialize_metadata(meta):\n",
    "    \"\"\"Recursively convert metadata to JSON-serializable format\"\"\"\n",
    "    def _serialize(obj):\n",
    "        if isinstance(obj, (str, int, float, bool, type(None))):\n",
    "            return obj\n",
    "        elif isinstance(obj, dict):\n",
    "            return {k: _serialize(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            return [_serialize(v) for v in obj]\n",
    "        else:\n",
    "            return str(obj)\n",
    "    return _serialize(meta)\n",
    "\n",
    "def save_stack_hdf5(directory, output_file=\"fast_stack.h5\"):\n",
    "    with h5py.File(os.path.join(directory, output_file), \"w\") as hf:\n",
    "        # Get sorted list of valid files\n",
    "        valid_files = sorted([\n",
    "            f for f in os.listdir(directory)\n",
    "            if f.endswith('.ndata1') and 'SuperScan (HAADF) (Gaussian Blur)' not in f\n",
    "        ])\n",
    "        num_images = len(valid_files)\n",
    "\n",
    "        # Create compound datatype\n",
    "        dt = np.dtype([\n",
    "            ('image', h5py.vlen_dtype(np.float32)),  # Adjust dtype as needed\n",
    "            ('metadata', h5py.string_dtype()),\n",
    "            ('shape_0', np.int32),\n",
    "            ('shape_1', np.int32)\n",
    "        ])\n",
    "\n",
    "        # Create single dataset for all images\n",
    "        ds = hf.create_dataset(\"image_stack\", (num_images,), \n",
    "                             dtype=dt, compression=\"gzip\")\n",
    "\n",
    "        for idx, filename in enumerate(valid_files):\n",
    "            # Load data\n",
    "            imageseq = ImageSequence(os.path.join(directory, filename))\n",
    "            img = imageseq.raw_data.astype(np.float32)\n",
    "            orig_shape = img.shape\n",
    "            \n",
    "            # Process metadata\n",
    "            try:\n",
    "                meta = serialize_metadata(imageseq.raw_metadata)\n",
    "            except AttributeError:\n",
    "                meta = {\"status\": \"no_metadata\"}\n",
    "            \n",
    "            # Store in compound dataset\n",
    "            ds[idx] = (\n",
    "                img.flatten(),  # vlen array\n",
    "                json.dumps(meta),\n",
    "                orig_shape[0],\n",
    "                orig_shape[1]\n",
    "            )\n",
    "\n",
    "def load_full_stack(h5_file):\n",
    "    \"\"\"Load all data in ~2x faster than group-based approach\"\"\"\n",
    "    with h5py.File(h5_file, \"r\") as hf:\n",
    "        ds = hf[\"image_stack\"]\n",
    "        # Single read operation for all data\n",
    "        all_data = ds[:]\n",
    "        \n",
    "    # Reconstruct images and metadata\n",
    "    images = [\n",
    "        arr.reshape((shape0, shape1)) \n",
    "        for arr, _, shape0, shape1 in all_data\n",
    "    ]\n",
    "    metadata = [\n",
    "        json.loads(meta) \n",
    "        for _, meta, _, _ in all_data\n",
    "    ]\n",
    "    return images, metadata\n",
    "\n",
    "\n",
    "def process_directory(directory):\n",
    "    \"\"\"Recursive directory processor\"\"\"\n",
    "    if any(f.endswith('.ndata1') for f in os.listdir(directory)):\n",
    "        save_stack_hdf5(directory)\n",
    "    else:\n",
    "        for subdir in os.listdir(directory):\n",
    "            subdir_path = os.path.join(directory, subdir)\n",
    "            if os.path.isdir(subdir_path):\n",
    "                process_directory(subdir_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # process_directory('/home/somar/Desktop/2025/Data for publication/Multilayer graphene/')\n",
    "    img, meta = load_full_stack(\"fast_stack.h5\")\n",
    "    print(img[100].shape, meta[100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion script (PKL -> HDF5)\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "from imagesequence import ImageSequence\n",
    "\n",
    "\n",
    "stacks = ['/home/somar/Desktop/2025/Data for publication/Sample 2344/ADF images/2024-12-19_After_cleaning_280C_2h/stack.pkl',\n",
    "            '/home/somar/Desktop/2025/Data for publication/Sample 2344/ADF images/2024-12-19_After_Plasma_irradiation/stack.pkl',\n",
    "            '/home/somar/Desktop/2025/Data for publication/Sample 2344/ADF images/2024-12-23_After_LaserCleaning_PtEvaporation/stack.pkl',\n",
    "            '/home/somar/Desktop/2025/Data for publication/Sample 2344/ADF images/After_Heating_200C/2025-01-08_After_Heating_200C/stack.pkl',\n",
    "            '/home/somar/Desktop/2025/Data for publication/Sample 2344/ADF images/After_Heating_200C/2025-01-20_After_Heating_200C/stack.pkl',\n",
    "            '/home/somar/Desktop/2025/Data for publication/Sample 2344/ADF images/After_Heating_150C/2025-02-06_After_Heating_150C/stack.pkl']\n",
    "\n",
    "metadata_s= ['/home/somar/Desktop/2025/Data for publication/Sample 2344/ADF images/2024-12-19_After_cleaning_280C_2h/metadata.pkl',\n",
    "            '/home/somar/Desktop/2025/Data for publication/Sample 2344/ADF images/2024-12-19_After_Plasma_irradiation/metadata.pkl',\n",
    "            '/home/somar/Desktop/2025/Data for publication/Sample 2344/ADF images/2024-12-23_After_LaserCleaning_PtEvaporation/metadata.pkl',\n",
    "            '/home/somar/Desktop/2025/Data for publication/Sample 2344/ADF images/After_Heating_200C/2025-01-08_After_Heating_200C/metadata.pkl',\n",
    "            '/home/somar/Desktop/2025/Data for publication/Sample 2344/ADF images/After_Heating_200C/2025-01-20_After_Heating_200C/metadata.pkl',\n",
    "            '/home/somar/Desktop/2025/Data for publication/Sample 2344/ADF images/After_Heating_150C/2025-02-06_After_Heating_150C/metadata.pkl']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H5ImageSequence:\n",
    "    def __init__(self, h5_file):\n",
    "        self.h5_file = h5_file\n",
    "        self._file = None  # Keep file handle open\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self._file = h5py.File(self.h5_file, 'r')\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self):\n",
    "        if self._file:\n",
    "            self._file.close()\n",
    "            \n",
    "    @property\n",
    "    def raw_data(self):\n",
    "        return H5LazyLoader(self.h5_file, 'images')\n",
    "    \n",
    "    @property\n",
    "    def raw_metadata(self):\n",
    "        return H5LazyLoader(self.h5_file, 'metadata')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class H5LazyLoader:\n",
    "    def __init__(self, h5_file, group):\n",
    "        self.h5_file = h5_file\n",
    "        self.group = group\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        with h5py.File(self.h5_file, 'r') as hf:\n",
    "            if self.group == 'images':\n",
    "                return hf[self.group][f\"image_{index:04d}\"][:]\n",
    "            else:  # metadata\n",
    "                return json.loads(hf[self.group][index].asstr()[()])\n",
    "    \n",
    "    def __len__(self):\n",
    "        with h5py.File(self.h5_file, 'r') as hf:\n",
    "            return len(hf[self.group])\n",
    "        \n",
    "    def get_metadata(self, target_key, data=None):\n",
    "        \"\"\"This method gets all the values of a certain target key in the metadata json file.\"\"\"\n",
    "        extracted_values = []\n",
    "        if data is None:\n",
    "            data = self.raw_metadata\n",
    "        if isinstance(data, dict):\n",
    "            if target_key in data and data[target_key] is not None:\n",
    "                value_to_extract = data.get(target_key)\n",
    "                extracted_values.append(value_to_extract)\n",
    "            for key, value in data.items():\n",
    "                if isinstance(value, (list, dict)):\n",
    "                    extracted_values.extend(self.get_metadata(target_key, value))\n",
    "        elif isinstance(data, list):\n",
    "            for item in data:\n",
    "                extracted_values.extend(self.get_metadata(target_key, item))\n",
    "        \n",
    "        if len(extracted_values) == 1 and isinstance(extracted_values[0], (list)):\n",
    "            return extracted_values[0]\n",
    "        return extracted_values\n",
    "    \n",
    "\n",
    "    def get_specific_metadata(self, target_key, required_keys=None, data=None, under_required_keys=False):\n",
    "        \"\"\"\n",
    "        This method gets specific metadata based on a condition that the target key is extracted only \n",
    "        when under specific required keys in the nested dictionaries. When providing required keys that are \n",
    "        at the same branch of the nested dictionary, the method will extract the target key for the one at higher level.\n",
    "        So either provide required keys that are at different branches or provide one required key.\n",
    "        \"\"\"\n",
    "        if data is None:\n",
    "            data = self.raw_metadata\n",
    "        if required_keys is None:\n",
    "            extracted_values = self.get_metadata(target_key, data)\n",
    "        else:\n",
    "            extracted_values = []\n",
    "            if isinstance(data, dict):\n",
    "                for key, value in data.items():\n",
    "                    if key in required_keys:\n",
    "                        # We are entering a required key scope\n",
    "                        if isinstance(value, dict):\n",
    "                            extracted_values.extend(self.get_specific_metadata(target_key, required_keys, value, True))\n",
    "                        elif isinstance(value, list):\n",
    "                            for item in value:\n",
    "                                extracted_values.extend(self.get_specific_metadata(target_key, required_keys, item, True))\n",
    "                    elif under_required_keys and key == target_key:\n",
    "                        extracted_values.append(value)\n",
    "                    else:\n",
    "                        if isinstance(value, (dict, list)):\n",
    "                            extracted_values.extend(self.get_specific_metadata(target_key, required_keys, value, under_required_keys))\n",
    "\n",
    "            elif isinstance(data, list):\n",
    "                for item in data:\n",
    "                    extracted_values.extend(self.get_specific_metadata(target_key, required_keys, item, under_required_keys))\n",
    "\n",
    "        if len(extracted_values) == 1 and isinstance(extracted_values[0], (list)):\n",
    "            return extracted_values[0]\n",
    "        return extracted_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import h5py\n",
    "from imagesequence import ImageSequence, H5ImageSequence\n",
    "# Example usage\n",
    "stack_h5 = '/home/somar/Desktop/own_stuff/imageanalysis/imageanalysis/src/stack.h5'\n",
    "stack = H5ImageSequence(stack_h5)\n",
    "metadata = stack.raw_metadata  # Shared instance for H5 files\n",
    "indices = [0, 1, 2, 3, 4, 5]  # Example indices to load\n",
    "data = stack.raw_data  # Shared instance for H5 files\n",
    "meta = metadata[f\"metadata_{1:04d}\"]\n",
    "print(meta)\n",
    "metadata.get_specific_metadata('fov_nm', required_keys=['scan_device_parameters'], data=meta)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt5\n",
    "from image_analysis_lastversion03_04_2025 import InteractiveImageAnalysis\n",
    "directories = ['/home/somar/Desktop/2025/Data for publication/Sample 2438/ADF images',\n",
    "               '/home/somar/Desktop/2025/Data for publication/Sample 2473/ADF images',\n",
    "               '/home/somar/Desktop/2025/Data for publication/Sample 2474/ADF images',\n",
    "               '/home/somar/Desktop/2025/Data for publication/Sample 2475/ADF images']\n",
    "\n",
    "# Usage example\n",
    "stacks_path = [stack + '/stack.pkl' for stack in directories][1]\n",
    "metadata_path = [stack + '/metadata.pkl' for stack in directories][1]\n",
    "font_path = \"/home/somar/.fonts/SourceSansPro-Semibold.otf\" \n",
    "stack_h5 = '/home/somar/Desktop/own_stuff/imageanalysis/imageanalysis/src/stack.h5'\n",
    "stack_h5_1 = '/home/somar/Desktop/2025/Data for publication/Sample 2344/ADF images/2024-12-23_After_LaserCleaning_PtEvaporation/stack.h5'\n",
    "image_analysis = InteractiveImageAnalysis(stack_h5_1, metadata_path=None, analysing_features=True, save_images_with_calibrated_scalebar=False, \n",
    "                                        clean_graphene_analysis=True, contamination_analysis=False, fixed_length_scalebar=True, font_path=font_path, \n",
    "                                        clusters_analysis=True, defects_analysis=False)\n",
    "\n",
    "    # This version is based on the display_image_with_scale_bar.ipynb notebook coppied at 03-04-2025 at 4:16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagesequence import ImageSequence\n",
    "stack_h5 = '/home/somar/Desktop/own_stuff/imageanalysis/imageanalysis/src/stack.h5'\n",
    "image = ImageSequence(stack_h5)\n",
    "# image.raw_data\n",
    "len(image.raw_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.read_csv('tst.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
